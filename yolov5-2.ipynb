{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start from a working python environment with Python>=3.8 and PyTorch>=1.6 installed, as well as pyyaml>=5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python pillow pyyaml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     19904  models.common.BottleneckCSP             [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    641792  models.common.BottleneckCSP             [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    378624  models.common.BottleneckCSP             [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     95104  models.common.BottleneckCSP             [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    313088  models.common.BottleneckCSP             [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1248768  models.common.BottleneckCSP             [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7468157 parameters, 7468157 gradients\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).autoshape()  # for PIL/cv2/np inputs and NMS\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'custom', path_or_model='yolov5s_voc_best.pt')\n",
    "#model = model.autoshape()  # for PIL/cv2/np inputs and NMS\n",
    "\n",
    "#model = torch.load('yolov5s.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RuntimeError: No such operator torchvision::nms\n",
    "###!pip3 install torch==1.3.0 torchvision==0.4.1 -f https://download.pytorch.org/whl/torch_stable.html -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nms algorithm process\n",
    "\n",
    "Sort the scores of all the boxes, select the highest score and its corresponding box\n",
    "Traverse the remaining boxes, if the overlap area (IOU) with the current highest partition is greater than a certain threshold, we will delete the box\n",
    "Continue to select the highest score in the unprocessed box and repeat the above process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "#img = Image.open('zidane.jpg')  # PIL image\n",
    "#img = Image.open('penguins.png')  # PIL image\n",
    "img = Image.open('giraffe.jpg')  # PIL image\n",
    "\n",
    "# Inference\n",
    "results = model(img)  # includes NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1/1: 480x640 8 persons, 1 giraffes, 1 bowls, \n"
     ]
    }
   ],
   "source": [
    "results.print()  # print results to screen\n",
    "results.show()  # display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[5.96104e+02, 1.24493e+02, 6.39818e+02, 2.48508e+02, 8.52926e-01, 0.00000e+00],\n",
      "        [5.21928e+02, 1.14640e+02, 5.44473e+02, 1.92111e+02, 7.36817e-01, 0.00000e+00],\n",
      "        [1.47862e+01, 3.00136e+02, 7.07593e+01, 3.35578e+02, 7.18281e-01, 4.50000e+01],\n",
      "        [5.48104e+02, 1.08907e+02, 5.64844e+02, 1.44169e+02, 6.19003e-01, 0.00000e+00],\n",
      "        [4.67983e+02, 1.18124e+02, 4.87731e+02, 1.66900e+02, 5.65179e-01, 0.00000e+00],\n",
      "        [7.01576e+01, 5.09581e+01, 5.37976e+02, 4.80000e+02, 5.59174e-01, 2.30000e+01],\n",
      "        [4.44154e+02, 1.14748e+02, 4.63336e+02, 1.67679e+02, 4.91974e-01, 0.00000e+00],\n",
      "        [4.50113e+02, 9.93443e+01, 4.66735e+02, 1.22584e+02, 3.87598e-01, 0.00000e+00],\n",
      "        [3.07796e+02, 8.22395e+01, 3.23122e+02, 1.03742e+02, 3.41511e-01, 0.00000e+00],\n",
      "        [4.18543e+02, 1.04283e+02, 4.33076e+02, 1.27812e+02, 2.75122e-01, 0.00000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('\\n', results.xyxy[0])  # print img1 predictions\n",
    "#          x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence        class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
